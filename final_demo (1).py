# -*- coding: utf-8 -*-
"""Final_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Z6qQPrMOHaXpru1nWeXJqEJ5n4QFXjb
"""

!pip install ultralytics
from ultralytics import YOLO

pip install pytesseract

import cv2, pandas as pd, time, math, numpy as np, pytesseract

from google.colab.patches import cv2_imshow

rider_model_path = "yolov8s.pt"  # pretrained on COCO
helmet_model_path = "/content/helmet_post_aug.pt"
plate_model_path = "/content/license_best (2).pt"

# Load models
rider_model = YOLO(rider_model_path)
helmet_model = YOLO(helmet_model_path)
plate_model = YOLO(plate_model_path)

def is_rider(person_box, bike_box):
    px1, py1, px2, py2 = person_box
    bx1, by1, bx2, by2 = bike_box
    person_center_x = (px1 + px2) / 2
    return (bx1 < person_center_x < bx2) and (py2 > by1)

def crop_head(frame, person_box, top_frac=0.45):
    x1,y1,x2,y2 = map(int, person_box)
    h = y2 - y1
    return frame[y1:int(y1 + top_frac*h), x1:x2]

# video_path = "/content/helmet_demo_video.gif"
# cap = cv2.VideoCapture(video_path)

!pip install easyocr
import easyocr
reader = easyocr.Reader(lang_list=['en'])

DETECT_CONF = 0.45     # detection confidence threshold
MIN_BOX_AREA = 1500    # min area to accept a detection (tune for your image resolution)
PLATE_CONF = 0.18
HELMET_CONF = 0.6

## HELPER FUNCTIONS

def to_numpy(x):
    """Convert possible tensor/torch objects to numpy array or python float."""
    try:
        return x.cpu().numpy()
    except:
        return np.array(x)

def box_area(box):
    x1,y1,x2,y2 = box
    return max(0, x2-x1) * max(0, y2-y1)

def iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interW = max(0, xB - xA)
    interH = max(0, yB - yA)
    inter = interW * interH
    boxAArea = box_area(boxA)
    boxBArea = box_area(boxB)
    denom = float(boxAArea + boxBArea - inter + 1e-6)
    return inter / denom

def keep_top_n_nonoverlapping(boxes, confs, labels, max_overlap=0.5):
    """Given lists of boxes/conf/labels, return filtered lists keeping high-conf boxes and dropping heavy overlaps."""
    idxs = np.argsort(-np.array(confs))  # descending
    kept = []
    for i in idxs:
        box = boxes[i]
        bad = False
        for k in kept:
            if iou(box, k) > max_overlap:
                bad = True
                break
        if not bad:
            kept.append(box)
    # Build filtered lists preserving original order for kept boxes
    kept_boxes, kept_confs, kept_labels = [], [], []
    for i in range(len(boxes)):
        for kb in kept:
            if np.all(np.array(boxes[i]) == np.array(kb)):
                kept_boxes.append(boxes[i])
                kept_confs.append(confs[i])
                kept_labels.append(labels[i])
                break
    return kept_boxes, kept_confs, kept_labels

def is_rider(person_box, bike_box, tol_y=0.25):
    """Heuristic: person bottom should overlap or be just above bike bbox, and x-center inside bike bbox."""
    px1,py1,px2,py2 = person_box
    bx1,by1,bx2,by2 = bike_box
    person_cx = (px1+px2)/2
    person_bottom = py2
    horiz_ok = (bx1 <= person_cx <= bx2)
    bike_h = by2 - by1
    vert_ok = (person_bottom >= (by1 - tol_y * bike_h)) and (person_bottom <= (by2 + tol_y * bike_h))
    return horiz_ok and vert_ok

"""OCR PREPROCESSING"""

def preprocess_plate_crop(plate_img, upscale_factor=2.5):
    # 1) upscale
    h,w = plate_img.shape[:2]
    new_w, new_h = int(w * upscale_factor), int(h * upscale_factor)
    up = cv2.resize(plate_img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)

    # 2) convert to gray and CLAHE
    gray = cv2.cvtColor(up, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    cl = clahe.apply(gray)

    # 3) denoise & bilateral filter to preserve edges
    den = cv2.fastNlMeansDenoising(cl, None, h=10, templateWindowSize=7, searchWindowSize=21)
    den = cv2.bilateralFilter(den, 9, 75, 75)

    # 4) sharpen
    kernel = np.array([[0,-1,0],[-1,7,-1],[0,-1,0]])
    sharp = cv2.filter2D(den, -1, kernel)

    # 5) optional adaptive threshold (helps OCR sometimes)
    th = cv2.adaptiveThreshold(sharp, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                               cv2.THRESH_BINARY, 41, 10)
    return up, sharp, th

def detect_plate_local(img, bike_box, plate_model, conf=0.35, imgsz=1280, pad_ratio=0.25):
    """
    Detect license plates inside a padded region around the bike_box.
    Returns:
      plates: list of ((x1,y1,x2,y2), conf) in full-image coords
      padded_crop: the raw padded crop (numpy array) used for detection
      pad_offset: (sx, sy) top-left offset to convert crop coords -> full image coords
    Notes:
      - Do NOT pass a preprocessed tuple to plate_model.predict(). Pass the raw crop.
      - Running detection on a higher imgsz helps for small plates (slower).
    """
    bx1,by1,bx2,by2 = bike_box
    h_img, w_img = img.shape[:2]

    pad_x = int(pad_ratio * (bx2 - bx1))
    pad_y = int(pad_ratio * (by2 - by1))
    sx = max(0, bx1 - pad_x)
    sy = max(0, by1 - pad_y)
    ex = min(w_img, bx2 + pad_x)
    ey = min(h_img, by2 + pad_y)

    # raw padded crop (pass this to detector)
    padded_crop = img[sy:ey, sx:ex].copy()
    if padded_crop.size == 0:
        return [], None, (sx, sy)

    # Run plate detector on the padded crop (high imgsz recommended for small plates)
    # Note: pass the crop itself (not preprocessed tuple)
    res = plate_model.predict(source=padded_crop, conf=conf, imgsz=imgsz, verbose=False)[0]

    plates = []
    if res.boxes is not None and len(res.boxes) > 0:
        for i in range(len(res.boxes)):
            # get box in crop coordinates
            bb = res.boxes.xyxy[i]
            # convert to numpy if tensor
            bb = bb.cpu().numpy() if hasattr(bb, "cpu") else np.array(bb)
            c = float(res.boxes.conf[i].cpu().numpy()) if hasattr(res.boxes.conf[i], "cpu") else float(res.boxes.conf[i])
            # convert box coords from crop -> full image coords using (sx, sy)
            x1 = int(bb[0]) + sx
            y1 = int(bb[1]) + sy
            x2 = int(bb[2]) + sx
            y2 = int(bb[3]) + sy
            plates.append(((x1, y1, x2, y2), c))

    return plates, padded_crop, (sx, sy)

"""Final function"""

def process_image(img, save=True):
    if img is None:
        print("Could not read"); return None

    h_img, w_img = img.shape[:2]
    # 1) Detect persons + motorcycles using COCO model (class 0=person, 3=motorcycle)
    det = rider_model.predict(img, conf=DETECT_CONF, classes=[0,3], verbose=False)[0]

    persons, bikes = [], []
    if det.boxes is not None and len(det.boxes) > 0:
        for i in range(len(det.boxes)):
            box = to_numpy(det.boxes.xyxy[i])
            cls = int(to_numpy(det.boxes.cls[i]).item()) if hasattr(det.boxes.cls[i], "cpu") or hasattr(det.boxes.cls[i], "item") else int(det.boxes.cls[i])
            conf = float(to_numpy(det.boxes.conf[i]).item()) if hasattr(det.boxes.conf[i], "cpu") or hasattr(det.boxes.conf[i], "item") else float(det.boxes.conf[i])
            x1,y1,x2,y2 = map(int, box)
            area = box_area((x1,y1,x2,y2))
            if area < MIN_BOX_AREA:
                continue
            if cls == 0:
                persons.append(((x1,y1,x2,y2), conf))
            elif cls == 3:
                bikes.append(((x1,y1,x2,y2), conf))

    # 2) Pair persons with bikes to find riders
    riders = []  # each element = (person_box, bike_box)
    for pbox, pconf in persons:
        for bbox, bconf in bikes:
            if is_rider(pbox, bbox):
                riders.append((pbox, bbox))
                # break to avoid pairing person to multiple bikes
                break

    annotated = img.copy()

    # 3) For each rider, run helmet detection on rider crop
    for (pbox, bbox) in riders:
        px1,py1,px2,py2 = pbox
        pad = 8
        rider_crop = img[max(0, py1-pad):py2+pad, max(0, px1-pad):px2+pad]
        if rider_crop.size == 0:
            continue

        # Run helmet model on rider crop
        hres = helmet_model.predict(rider_crop, conf=HELMET_CONF, verbose=False)[0]

        # Collect boxes/conf/labels from hres, convert to image coords (rider_crop -> img)
        h_boxes, h_confs, h_labels = [], [], []
        if hres.boxes is not None and len(hres.boxes) > 0:
            for i in range(len(hres.boxes)):
                b = to_numpy(hres.boxes.xyxy[i])  # box relative to crop
                c = float(to_numpy(hres.boxes.conf[i]).item())
                cls_id = int(to_numpy(hres.boxes.cls[i]).item())
                # convert to original img coords
                x1 = int(b[0]) + px1
                y1 = int(b[1]) + py1
                x2 = int(b[2]) + px1
                y2 = int(b[3]) + py1
                if box_area((x1,y1,x2,y2)) < MIN_BOX_AREA:
                    continue
                h_boxes.append((x1,y1,x2,y2))
                h_confs.append(c)
                # robust label name
                name = helmet_model.model.names[cls_id] if hasattr(helmet_model, "model") else helmet_model.names[cls_id]
                h_labels.append(str(name))

        # If no helmet boxes found by model, treat as "No Helmet" candidate
        helmet_label = "Unknown"
        helmet_conf = 0.0

        if len(h_boxes) > 0:
            # keep top non-overlapping boxes
            h_boxes, h_confs, h_labels = keep_top_n_nonoverlapping(h_boxes, h_confs, h_labels, max_overlap=0.5)
            # pick highest confidence
            top_idx = int(np.argmax(h_confs))
            helmet_label = h_labels[top_idx]
            helmet_conf = h_confs[top_idx]
        else:
            # No detection: assume no helmet (you may optionally treat as unknown)
            helmet_label = "No Helmet"
            helmet_conf = 0.0

        # normalize label text
        lab_lower = helmet_label.lower()

        # color logic (check "without" before "with")
        if "without" in lab_lower or "no helmet" in lab_lower or "no-helmet" in lab_lower:
            color = (0,0,255)   # red
        elif "with" in lab_lower or "with helmet" in lab_lower:
            color = (0,255,0)   # green
        else:
            # if ambiguous, choose red if conf low
            color = (0,255,0) if helmet_conf >= 0.5 else (0,0,255)

        # draw rider box and label
        cv2.rectangle(annotated, (px1, py1), (px2, py2), color, 3)
        labtxt = f"{helmet_label} ({helmet_conf:.2f})"
        cv2.putText(annotated, labtxt, (px1, max(py1-8,12)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

        plate_text = "N/A"
        # 4) If no helmet, run plate detection on the whole image (or localized region)
        # --- Step 4: If no helmet → detect plate + OCR (using helper functions) ---

        if "without" in lab_lower or "no helmet" in lab_lower or "no-helmet" in lab_lower:
           plates, padded_crop, _ = detect_plate_local(img, bbox, plate_model, conf=PLATE_CONF, imgsz=1120)
           plate_text = "Unreadable"  # default fallback

           if plates and len(plates) > 0:
        # sort by confidence
             plates = sorted(plates, key=lambda x: x[1], reverse=True)
             for (bx1, by1, bx2, by2), pconf in plates:

               plate_crop = img[by1:by2, bx1:bx2]

               try:
                 up, sharp, th = preprocess_plate_crop(plate_crop)
                 result = pytesseract.image_to_string(up)

                 if result and len(result) > 0:
                   plate_text = " ".join([r[1] for r in result])
                 else:
                   plate_text = "Unreadable"
               except Exception as e:
                 print("OCR failed:", e)
                 plate_text = "Unreadable"

        # Draw bounding box and label
               cv2.rectangle(annotated, (bx1, by1), (bx2, by2), (255, 0, 0), 2)

        #   cv2.putText(
        #     annotated,
        #     plate_text if plate_text else "Unreadable",
        #     (bx1, max(by1 - 6, 12)),
        #     cv2.FONT_HERSHEY_SIMPLEX,
        #     0.6,
        #     (255, 200, 0),
        #     2,
        # )

               print(f"Plate OCR: {plate_text} (conf={pconf:.2f})")
           else:
             print("No plates detected")


    return annotated

img = cv2.imread('/content/img6.jpg')
cv2_imshow(process_image(img))

img2 = cv2.imread('/content/img2.jpeg.jpg')
cv2_imshow(process_image(img2))

img3 = cv2.imread('/content/pappu_img3.jpg')
cv2_imshow(process_image(img3))

img2.shape

img3.shape

img4 = cv2.imread('/content/img1.jpg')
cv2_imshow(process_image(img4))

img4.shape

img5 = cv2.imread('/content/img5.jpg')
cv2_imshow(process_image(img5))

img = cv2.imread('/content/img10.jpg')
cv2_imshow(process_image(img))

img12 = cv2.imread('/content/img10.png')
cv2_imshow(process_image(img12))

import cv2

# Open input video
cap = cv2.VideoCapture("/content/helmet_demo_video.gif")
fps = cap.get(cv2.CAP_PROP_FPS)
width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Create output writer
out = cv2.VideoWriter(
    "output_annotated.mp4",
    cv2.VideoWriter_fourcc(*"mp4v"),
    fps,
    (width, height)
)

frame_count = 0
while True:
    ret, frame = cap.read()
    if not ret:
        break

    frame_count += 1

    # Optionally skip frames for speed
    if frame_count % 2 != 0:
        continue

    try:
        #Call existing image-processing function
        annotated = process_image(frame)
    except Exception as e:
        print(f"Error on frame {frame_count}: {e}")
        annotated = frame  # fallback: write original frame

    out.write(annotated)

    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
out.release()
cv2.destroyAllWindows()
print("✅ Video saved as output_annotated.mp4")